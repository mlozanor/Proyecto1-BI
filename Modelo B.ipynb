{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo B \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por: Catalina Alvarez "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\catalina\\anaconda3\\lib\\site-packages (7.5.0)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from inflect) (10.6.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from inflect) (4.4.2)\n",
      "Requirement already satisfied: typing_extensions>=4.10.0 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n",
      "Requirement already satisfied: importlib_metadata>=3.6 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from importlib_metadata>=3.6->typeguard>=4.0.1->inflect) (3.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\catalina\\anaconda3\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\catalina\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\catalina\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect\n",
    "!pip install --upgrade wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Catalina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparaci√≥n del Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Catalina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Catalina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "#from num2words import num2words\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import inflect\n",
    "import joblib\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con S√°nchez: 'Europa necesit...</td>\n",
       "      <td>El diario brit√°nico public√≥ este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castell√≥ su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...</td>\n",
       "      <td>En pol√≠tica, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No ser√≠a ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el l√≠der de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con S√°nchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No ser√≠a ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario brit√°nico public√≥ este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castell√≥ su fiesta...  25/04/2022  \n",
       "3  En pol√≠tica, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el l√≠der de...  09/03/2018  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original = pd.read_csv(r'C:\\Users\\Catalina\\OneDrive\\Documents\\UNIANDES\\6SEMESTRE\\BI\\Proyecto1-BI-1\\Etapa 1\\datos\\fake_news_spanish.csv', sep=';', encoding='utf-8')\n",
    "df_original.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con S√°nchez: 'Europa necesit...</td>\n",
       "      <td>El diario brit√°nico public√≥ este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castell√≥ su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...</td>\n",
       "      <td>En pol√≠tica, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No ser√≠a ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el l√≠der de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con S√°nchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No ser√≠a ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario brit√°nico public√≥ este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castell√≥ su fiesta...  25/04/2022  \n",
       "3  En pol√≠tica, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el l√≠der de...  09/03/2018  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df_original.copy()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calidad de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se van a analizar 4 dimensiones distintas con respecto a la calidad de los datos (Completitud, Validez, Unicidad y Consistencia)\n",
    "\n",
    "# Completitud\n",
    "\n",
    "Bajo una inspeccion sencilla, parece no haber problemas de completitud en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              0\n",
       "Label           0\n",
       "Titulo         16\n",
       "Descripcion     0\n",
       "Fecha           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             57063\n",
       "Label          57063\n",
       "Titulo         57047\n",
       "Descripcion    57063\n",
       "Fecha          57063\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    33158\n",
       "0    23905\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip all values in the column\n",
    "train['Descripcion'] = train['Descripcion'].str.strip()\n",
    "# count all values in the column where the value is an empty string\n",
    "train['Descripcion'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validez \n",
    "Se identifican los siguientes problemas de validez: \n",
    "Los textos en el dataset tienen problemas, vocales con tilde han sido remplazados por caracteres no validos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(text):\n",
    "    replacements = {\n",
    "        '√É¬°': '√°',\n",
    "        '√É¬©': '√©',\n",
    "        '√É¬≠': '√≠',\n",
    "        '√É¬≥': '√≥',\n",
    "        '√É¬∫': '√∫',\n",
    "        '√É¬±': '√±'\n",
    "    }\n",
    "    \n",
    "    for wrong, correct in replacements.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    \n",
    "    return text\n",
    "\n",
    "train['Descripcion'] = train['Descripcion'].apply(fix_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir la columna Titulo de float a string para aplicar los cambios de fix_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir la columna \"Titulo\" de float a string\n",
    "train[\"Titulo\"] = train[\"Titulo\"].astype(str)\n",
    "\n",
    "# Verificar el cambio de tipo de dato\n",
    "train.dtypes[\"Titulo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Titulo'] = train['Titulo'].apply(fix_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con S√°nchez: 'Europa necesit...</td>\n",
       "      <td>El diario brit√°nico public√≥ este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castell√≥ su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...</td>\n",
       "      <td>En pol√≠tica, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No ser√≠a ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el l√≠der de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con S√°nchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No ser√≠a ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario brit√°nico public√≥ este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACI√ìN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castell√≥ su fiesta...  25/04/2022  \n",
       "3  En pol√≠tica, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el l√≠der de...  09/03/2018  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicidad \n",
    "En primera instancia, el dataset no tiene columnas unicas sigueindo alguna restriccion de negocio. Luego de una sencilla inspeccion, parece haber filas totalmente duplicadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistencia \n",
    "La coluumna \"ID\" no es unica en su totalidad, es posible que haya inconsistencias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"ID\"].nunique() == len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfilamiento de datos \n",
    "\n",
    "Eliminacion de datos duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             56618\n",
       "Label          56618\n",
       "Titulo         56618\n",
       "Descripcion    56618\n",
       "Fecha          56618\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_grafico1(palabras, stopwords):\n",
    "    if palabras.empty:\n",
    "        print(\"No hay palabras para mostrar en la nube.\")\n",
    "        return\n",
    "    \n",
    "    # Convertir todo a string y eliminar NaN\n",
    "    palabras = palabras.dropna().astype(str)\n",
    "    \n",
    "    # Concatenar todas las palabras en un solo string\n",
    "    comment_words = \" \".join(palabras)\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=600, height=600,\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        min_font_size=10\n",
    "    ).generate(comment_words)\n",
    "\n",
    "    # Mostrar la imagen\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fuente seleccionada: C:/Windows/Fonts/arial.ttf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lista de posibles fuentes compatibles\n",
    "possible_fonts = [\n",
    "    \"C:/Windows/Fonts/arial.ttf\",  # Windows\n",
    "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",  # Linux\n",
    "    \"/System/Library/Fonts/Supplemental/Arial.ttf\"  # MacOS\n",
    "]\n",
    "\n",
    "# Buscar la primera fuente disponible en el sistema\n",
    "font_path = None\n",
    "for font in possible_fonts:\n",
    "    if os.path.exists(font):\n",
    "        font_path = font\n",
    "        break\n",
    "\n",
    "if font_path is None:\n",
    "    raise ValueError(\"‚ö†Ô∏è No se encontr√≥ una fuente TrueType v√°lida en el sistema. Prueba especificar una manualmente.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Fuente seleccionada: {font_path}\")\n",
    "# Verificar qu√© fuente se est√° usando\n",
    "def mostrar_grafico1(palabras, stopwords=[]):\n",
    "    if palabras.empty:\n",
    "        print(\"No hay palabras para mostrar en la nube.\")\n",
    "        return\n",
    "\n",
    "    palabras = palabras.dropna().astype(str)\n",
    "    comment_words = \" \".join(palabras)\n",
    "\n",
    "    # üîπ Verificar que la fuente se est√° usando\n",
    "    print(f\"Usando la fuente para WordCloud: {font_path}\")\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "    width=600, height=600,\n",
    "    background_color='white',\n",
    "    stopwords=stopwords,\n",
    "    min_font_size=10\n",
    ").generate(comment_words)\n",
    "\n",
    "\n",
    "    # Mostrar la imagen de la nube de palabras\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta de la fuente utilizada: C:/Windows/Fonts/arial.ttf\n",
      "¬øLa fuente existe?: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Ruta de la fuente utilizada: {font_path}\")\n",
    "print(f\"¬øLa fuente existe?: {os.path.exists(font_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train['Titulo'].isnull().sum())  # Contar valores nulos\n",
    "print(train['Titulo'].apply(lambda x: isinstance(x, float)).sum())  # Contar valores num√©ricos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion, se ilustrara que palabras suelen repetirse y encontrarse en los Label's 0 y 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Words for class: 1 ----------\n",
      "Total de t√≠tulos en esta clase: 32738\n",
      "Primeros valores en la clase 1: 0    'The Guardian' va con S√°nchez: 'Europa necesit...\n",
      "2    El 'Ahora o nunca' de Joan Fuster sobre el est...\n",
      "3    Iglesias alienta a Yolanda D√≠az, ERC y EH Bild...\n",
      "5    El PNV consolida su mayor√≠a, el PSE salva los ...\n",
      "7    La Fiscal√≠a pide prisi√≥n incondicional para lo...\n",
      "Name: Titulo, dtype: object\n",
      "Usando la fuente para WordCloud: C:/Windows/Fonts/arial.ttf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Catalina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Verificar si `palabras` contiene valores extra√±os\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimeros valores en la clase \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, palabras\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m---> 28\u001b[0m \u001b[43mmostrar_grafico1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalabras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mmostrar_grafico1\u001b[1;34m(palabras, stopwords)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# üîπ Verificar que la fuente se est√° usando\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsando la fuente para WordCloud: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfont_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     40\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Mostrar la imagen de la nube de palabras\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m), facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Catalina\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Catalina\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:624\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 624\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Catalina\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m     font_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrequencies\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmax_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# find font sizes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout_]\n",
      "File \u001b[1;32mc:\\Users\\Catalina\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:511\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    508\u001b[0m transposed_font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mTransposedFont(\n\u001b[0;32m    509\u001b[0m     font, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m box_size \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransposed_font\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m occupancy\u001b[38;5;241m.\u001b[39msample_position(box_size[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[0;32m    514\u001b[0m                                    box_size[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[0;32m    515\u001b[0m                                    random_state)\n",
      "File \u001b[1;32mc:\\Users\\Catalina\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m     font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetfont()\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(font, ImageFont\u001b[38;5;241m.\u001b[39mFreeTypeFont):\n\u001b[1;32m--> 671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly supported for TrueType fonts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedded_color \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfontmode\n\u001b[0;32m    673\u001b[0m bbox \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mgetbbox(\n\u001b[0;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[0;32m    675\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stopwords si no est√°n disponibles\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Definir stopwords en espa√±ol\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Asegurar que 'Titulo' no tiene valores nulos ni num√©ricos\n",
    "train['Titulo'] = train['Titulo'].fillna(\"\").astype(str)\n",
    "\n",
    "for i in train['Label'].unique():\n",
    "    print(f'---------- Words for class: {i} ----------')\n",
    "\n",
    "    # Filtrar la columna 'Titulo' asegurando que solo contiene texto\n",
    "    palabras = train.loc[train['Label'] == i, 'Titulo'].dropna().astype(str)\n",
    "\n",
    "    print(f\"Total de t√≠tulos en esta clase: {len(palabras)}\")\n",
    "\n",
    "    if palabras.empty:\n",
    "        print(f\"No hay t√≠tulos para la clase {i}, omitiendo...\")\n",
    "        continue  # Saltar iteraci√≥n si no hay datos\n",
    "\n",
    "    # Verificar si `palabras` contiene valores extra√±os\n",
    "    print(f\"Primeros valores en la clase {i}:\", palabras.head())\n",
    "\n",
    "    mostrar_grafico1(palabras, stopwords=stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['Titulo'], train['Label'] \n",
    "display(X_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modificacion de Caracteres\n",
    "\n",
    "Se pasan todos los caracteres a minusculas, se eliminan textos de puntuacion y stop words (suelen ser articulos y conectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cambiar minusculas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(words: list[str]):\n",
    "    return [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Quitar puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words: list[str]):\n",
    "    return [word for word in words if word.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Eliminar StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words: list[str]):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "    return [word for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(words):\n",
    "    words = to_lower(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se gace un procedimiento de Tokenizacion para desglozar palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = X_train.apply(word_tokenize)\n",
    "new_X_train = new_X_train.apply(preprocessing) \n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_and_lemmatize(words):\n",
    "    #words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = new_X_train.apply(stem_and_lemmatize) \n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separaci√≥n Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Definir variables de entrada (X) y salida (y)\n",
    "X = df_original['Titulo']  # Se usar√° el t√≠tulo de las noticias como input\n",
    "y = df_original['Label']  # La etiqueta de la noticia es la variable de salida\n",
    "\n",
    "# Divisi√≥n de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuenta de labels por cada conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_dict = dict(Counter(y_train))\n",
    "test_dict = dict(Counter(y_test))\n",
    "\n",
    "# Mostrar conteo de etiquetas para train y test\n",
    "pd.DataFrame({'Train': train_dict, 'Test': test_dict})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizaci√≥n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fill missing values with empty string\n",
    "X_train = X_train.fillna('')\n",
    "X_test = X_test.fillna('')\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "X_train_features = cv.fit_transform(X_train)\n",
    "X_test_features = cv.transform(X_test)\n",
    "X_train_features.shape, X_test_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1.)\n",
    "mnb.fit(X_train_features, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import cloudpickle\n",
    "\n",
    "if 'pipeline_model' in locals():\n",
    "    serialized = cloudpickle.dumps(pipeline_model)\n",
    "    with open('pipeline_model.pkl', 'wb') as f:\n",
    "        f.write(serialized)\n",
    "    print(\"Modelo guardado correctamente.\")\n",
    "else:\n",
    "    print(\"Error: No se pudo guardar el modelo porque 'pipeline_model' no est√° definido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y_pred = mnb.predict(X_test_features)\n",
    "y_pred_train = mnb.predict(X_train_features)\n",
    "\n",
    "print('Accuracy (Train):', metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Accuracy (Test):', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "mnb_cv_scores = cross_val_score(mnb, X_train_features, y_train, cv=5)\n",
    "mnb_cv_mean_score = np.mean(mnb_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_cv_mean_score)\n",
    "\n",
    "print(\"\\n---- Classification Report (Train) ----\\n\")\n",
    "print(metrics.classification_report(y_train, y_pred_train))\n",
    "\n",
    "print(\"\\n---- Classification Report (Test) ----\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaci√≥n de la Matriz de Confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "train_cm = confusion_matrix(y_train, y_pred_train, labels=mnb.classes_)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=train_cm, display_labels=mnb.classes_)\n",
    "axes[0].set_title('Train')\n",
    "disp_train.plot(ax=axes[0])\n",
    "\n",
    "test_cm = confusion_matrix(y_test, y_pred, labels=mnb.classes_)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=test_cm, display_labels=mnb.classes_)\n",
    "axes[1].set_title('Test')\n",
    "disp_test.plot(ax=axes[1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizaci√≥n del modelo con GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mnb_pipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'cv__min_df': [0.0, 0.1, 0.2],\n",
    "    'cv__max_df': [1.0, 0.9, 0.8],\n",
    "    'mnb__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]\n",
    "}\n",
    "\n",
    "gs_mnb = GridSearchCV(mnb_pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar mejores par√°metros\n",
    "cv_results = gs_mnb.cv_results_\n",
    "results_df = pd.DataFrame({'rank': cv_results['rank_test_score'],\n",
    "                           'params': cv_results['params'],\n",
    "                           'cv score (mean)': cv_results['mean_test_score'],\n",
    "                           'cv score (std)': cv_results['std_test_score']})\n",
    "results_df = results_df.sort_values(by='rank').head()\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "results_df\n",
    "\n",
    "best_mnb_test_score = gs_mnb.score(X_test, y_test)\n",
    "print('Test Accuracy :', best_mnb_test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creaci√≥n del Pipeline Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize_documents(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)  # Eliminar caracteres especiales\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    tokens = word_tokenize(doc)\n",
    "    doc = ' '.join(tokens)\n",
    "\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_documents)\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('normalize_corpus', FunctionTransformer(normalize_corpus)),\n",
    "    ('cv', CountVectorizer(min_df=0., max_df=1.)),\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('clf', MultinomialNB(alpha=1.))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "# Convertir valores nulos o float a string vac√≠a\n",
    "    df_original['Titulo'] = df_original['Titulo'].astype(str)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train['Titulo'], train['Label'], test_size=0.25, random_state=1)\n",
    "\n",
    "    vectorized_result = pipeline.named_steps['preprocessing'].fit_transform(X_train)\n",
    "    pipeline.named_steps['clf'].fit(vectorized_result, y_train)\n",
    "\n",
    "    pipeline_model = pipeline\n",
    "\n",
    "    y_pred = pipeline_model.predict(X_test)\n",
    "\n",
    "    mnb_cv_scores = cross_val_score(pipeline_model, X_train, y_train, cv=5)\n",
    "    mnb_cv_mean_score = np.mean(mnb_cv_scores)\n",
    "    print(f\"Cross Validation Score (5-fold): {mnb_cv_scores}\")\n",
    "    print(f\"Mean Cross Validation Score: {mnb_cv_mean_score}\")\n",
    "\n",
    "    print(f\"Accuracy: {metrics.accuracy_score(y_test, y_pred)}\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la ejecuci√≥n del pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializaci√≥n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import cloudpickle\n",
    "\n",
    "serialized = cloudpickle.dumps(pipeline_model)\n",
    "open('pipeline_model.pkl', 'wb').write(serialized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "font_dir = \"C:/Windows/Fonts\"\n",
    "fonts = [f for f in os.listdir(font_dir) if f.endswith(\".ttf\")]\n",
    "print(fonts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
